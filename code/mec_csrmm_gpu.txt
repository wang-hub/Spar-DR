##测试sparseMEC算子的te转tir
from functools import partial, reduce
import numpy as np
import tvm
from tvm.topi.utils import get_const_int, get_const_tuple, simplify, tag
from tvm.topi.nn.pad import pad
from tvm.topi.nn.utils import get_pad_tuple
from tvm import auto_scheduler
from tvm.topi.testing import conv2d_nchw_python
import tvm.testing
from tvm import te, auto_scheduler, runtime
from tvm.topi.sparse.utils import random_bsr_matrix
from tvm import IRModule
from tvm.contrib import tedd
from collections import namedtuple
import sys
sys.path.insert(0,sys.path[0]+'/../..')
import utils
import os
import numpy as np


@auto_scheduler.register_workload
# def mec_csrmm(data,weight_data,weight_indices_row,weight_indices_col,weight_indptr,resIdx,reshapeIdx,kernel_size,stride,padding,dilation=1,out_dtype = None):
def mec_csrmm(data_shape,weight_data_shape,weight_indices_shape,weight_indptr_shape,resIdx_shape,reshapeIdx_shape,kernel_size,stride,padding,dilation=1,out_dtype = None):
    # data = te.placeholder(data_shape,dtype='float32',name = 'input')
    weight_data = te.placeholder(weight_data_shape,dtype='float32',name='w_data')
    weight_indices_row = te.placeholder(weight_indices_shape,dtype='int32',name='w_indices_row')
    weight_indices_col = te.placeholder(weight_indices_shape,dtype='int32',name='w_indices_col')
    weight_indptr = te.placeholder(weight_indptr_shape,dtype='int32',name='w_indptr')
    resIdx = te.placeholder(resIdx_shape,dtype='int32',name='w_resIdx')
    reshapeIdx = te.placeholder(reshapeIdx_shape,dtype='int32',name='w_reshapeIdx')
    
    if out_dtype is None:
        out_dtype = weight_data.dtype
        # out_dtype = tranData.dtype
    assert isinstance(stride, int) or len(stride) == 2
    assert isinstance(dilation, int) or len(dilation) == 2
    if isinstance(stride, int):
        stride = stride_h = stride_w = stride
    else:
        stride_h, stride_w = stride
        stride = stride_h
    if isinstance(dilation, int):
        dilation_h = dilation_w = dilation
    else:
        dilation_h, dilation_w = dilation
    batch, in_channel, in_height, in_width = data_shape
    # compute the output shape
    dilated_kernel_h = (kernel_size - 1) * dilation_h + 1
    dilated_kernel_w = (kernel_size - 1) * dilation_w + 1
    pad_top, pad_left, pad_down, pad_right = get_pad_tuple(
        padding, (dilated_kernel_h, dilated_kernel_w)
    )
    out_height = simplify((in_height - dilated_kernel_h + pad_top + pad_down) // stride_h + 1)
    out_width = simplify((in_width - dilated_kernel_w + pad_left + pad_right) // stride_w + 1)
    # compute graph
    pad_before = [0, 0, pad_top, pad_left]
    pad_after = [0, 0, pad_down, pad_right]
    # temp = pad(data, pad_before, pad_after, name="pad_temp")
    # print("temp.shape:",temp.shape)
    tranData = te.placeholder((kernel_size*in_channel,out_height*(in_height+2*padding[0])),dtype='float32',name = 'input')
    # tranData = featureTrans(temp,kernel_size,stride)
    # print("trandata.shape:",tranData.shape)
    out_channel = get_const_int(weight_indptr.shape[0]) - 1
    # (n-k+2*p)//s+1
    oshape = (batch,out_channel,out_height, out_width)
    def f(n,row,h,w):
        # print("row:",row,end=' ')
        row = resIdx[row]
        # print(row)
        row_start = weight_indptr[row]
        row_end = weight_indptr[row + 1]
        row_elems = row_end - row_start
        elem_idx = te.reduce_axis((0, row_elems), name="elem_idx")
        elem = row_start + elem_idx
        a_val = weight_data[elem]
        #这里可能是影响速度的地方
        #可以预处理
        d_row = weight_indices_row[elem]
        d_col = weight_indices_col[elem]
        weight_val = tranData[d_row, d_col + h*out_height + w]
        return te.sum(a_val * weight_val, axis=elem_idx,)

    conv = te.compute(oshape, f, tag="mec_csrmm", name='mec_csrmm_k1')
    # output = te.compute(oshape,
    #                    lambda n,c,h,w:
    #                     conv[n,reshapeIdx[c],h,w],
    #                    name='conv_r',
    #                 )
    # return [output,temp,tranData]
    # return [data,weight_data, weight_indices_row, weight_indices_col, weight_indptr, resIdx, reshapeIdx,output,temp,tranData,conv]
    return [tranData,weight_data, weight_indices_row, weight_indices_col, weight_indptr, resIdx, reshapeIdx,conv]


#将n个数加和成m个，使m个数尽可能相等
def alor(L,m):
    n = len(L)
    # print(L)
    sorted_id = sorted(range(len(L)), key=lambda k: L[k], reverse=True)
    L = sorted(L, reverse=True)
    resNum = [0] * m
    resIdx = [[] for _ in range(m)]
    res = [[] for _ in range(m)]
    # print(L)
    # print(sorted_id)
    for i in range(n):
        min_index = resNum.index(min(resNum))
        # print(min_index)
        res[min_index].append(L[i])
        resNum[min_index] += L[i]
        resIdx[min_index].append(sorted_id[i])
    # print("resNum:",resNum)
    # print("res:",res)
    print('resIdx:',resIdx)
    return [resNum,res,resIdx]

def dealKernel(csr,para):
    # print(csr.indptr)
    #每个通道有多少数
    L = []
    for i in range(len(csr.indptr)-1):
        L.append((csr.indptr)[i+1] - (csr.indptr)[i])
    print(L)
    _,_,Idx = alor(L,para)
    resIdx = []
    for li in Idx:
        li.sort()
        resIdx += li
    #升序排索引
    reshapeIdx = sorted(range(len(resIdx)), key=lambda k: resIdx[k], )
    resIdx = np.array(resIdx).astype("int32")
    reshapeIdx = np.array(reshapeIdx).astype("int32")
    print("resIdx:",resIdx)
    print("reshapeIdx:",reshapeIdx)
    return resIdx,reshapeIdx



#返回运行时间
def conv(input_file=None,
         kernel_file=None,
         padding=(1,1),
         strides=(1,1),
         kISone = False):
    x = np.load(input_file)
    # print("x shape:",x.shape)
    N,CI,H,W = x.shape
    print("input shape:",x.shape)
    kernel = np.load(kernel_file)
    print("kernel shape:",kernel.shape)
    CO,CI,KH,KW = kernel.shape
    # N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 358, 358, 3, 3, (1, 1), (1, 1)
    kernel_size = KH
    # sparity = 0.9
    # para = 16
    para = 64
    # kernel_size*C,out_size*H
    # for para in [8,16,32,64]:
    csr = utils.deal_sp_kernel(kernel)
    resIdx,reshapeIdx = dealKernel(csr,para)
    out_size = utils.conv_out_size(H,kernel_size,padding[0],strides[0])
    Xtrans = np.random.randn(kernel_size*CI,out_size*(H+2*padding[0])).astype("float32")
    target = tvm.target.Target("cuda")
    dev = tvm.cuda(0)
    out = tvm.nd.empty((N,CO,out_size,out_size), device=dev)
    # pad_out = tvm.nd.empty((N,CI,H+2*padding[0],W+2*padding[1]), device=dev)
    # trans_out = tvm.nd.empty((kernel_size*CI,out_size*(H+2*padding[0])), device=dev)
    #预处理
    x_row = CI * kernel_size
    weight_row = csr.indices % x_row
    weight_col = csr.indices // x_row * out_size
    print("#####################参数输出############################")
    print('weight_data:',csr.data.shape)
    print('weight_row:',weight_row.shape)
    print('weight_col:',weight_col.shape)
    print('weight_indptr:',csr.indptr.shape)
    print('稀疏度：', csr.data.shape[0]/(float)(CO*CI*kernel_size*kernel_size))
    print('输出尺寸：',out.shape)

    #数据形状获取
    data_shape = x.shape
    weight_data_shape = csr.data.shape
    weight_indices_shape = csr.indices.shape
    weight_indptr_shape = csr.indptr.shape
    resIdx_shape = resIdx.shape
    reshapeIdx_shape = reshapeIdx.shape
    #函数
    # input_mec, weight_data, weight_indices_row, weight_indices_col, weight_indptr,weight_resIdx, weight_reshapeIdx,output,padData,transData,con = mec_csrmm(data_shape,
    input_mec, weight_data, weight_indices_row, weight_indices_col, weight_indptr,weight_resIdx, weight_reshapeIdx,conv = mec_csrmm(data_shape,
                                        weight_data_shape,
                                        weight_indices_shape,
                                        weight_indptr_shape,
                                        resIdx_shape,
                                        reshapeIdx_shape,
                                        kernel_size,
                                        strides,
                                        padding,
                                        dilation=1,)

    #调度conv
    # print("########################CONV调度#########################")
    # print("conv input tensor:",conv.op.input_tensors)
    # print("conv.op.axis:", type(conv.op.axis), conv.op.axis)
    # print("conv.op.reduce_axis:", type(conv.op.reduce_axis), conv.op.reduce_axis)

    output = conv
    # 获取 GPU 线程索引
    block_x = te.thread_axis("blockIdx.x")
    thread_x = te.thread_axis("threadIdx.x")

    #te创建调度
    s = te.create_schedule(output.op)


    nt = para
    outOP = conv
    x, y, z, zz = s[outOP].op.axis
    z = s[outOP].fuse(z, zz)
    y = s[outOP].fuse(y, z)
    fused = s[outOP].fuse(x, y)
    bx, tx = s[outOP].split(fused, factor=nt)
    s[outOP].bind(bx,block_x)
    s[outOP].bind(tx,thread_x)

    m = tvm.lower(s, [input_mec,
                    weight_data,
                    weight_indices_row,
                    weight_indices_col,
                    weight_indptr,
                    weight_resIdx,
                    weight_reshapeIdx,
                    #   output,
                    conv], name = 'test_mec')
    # print(m)

 
    #调度conv
    # print("########################CONV调度#########################")
    # print("conv input tensor:",conv.op.input_tensors)
    # print("conv.op.axis:", type(conv.op.axis), conv.op.axis)
    # print("conv.op.reduce_axis:", type(conv.op.reduce_axis), conv.op.reduce_axis)

    # print("线程绑定成功")
 
    mod = tvm.build(m, target=target)

    #数据转换
    # x_tvm,weight_indptr_tvm,weight_indices_row_tvm,weight_indices_col_tvm,weight_data_tvm,y = (tvm.nd.array(i,device=dev) for i in(x,csr.indptr,weight_row,weight_col,csr.data,out))
    x_tvm,weight_indptr_tvm,weight_indices_row_tvm,weight_indices_col_tvm,weight_data_tvm,y = (tvm.nd.array(i,device=dev) for i in(Xtrans,csr.indptr,weight_row,weight_col,csr.data,out))
    weight_row,weight_col = (tvm.nd.array(i,device=dev) for i in(weight_row,weight_col))

    resIdx_tvm = tvm.nd.array(resIdx,device=dev)
    reshapeIdx_tvm = tvm.nd.array(reshapeIdx,device=dev)
    # pad_out_tvm = tvm.nd.array(pad_out,device=dev)
    # trans_out_tvm = tvm.nd.array(trans_out,device=dev)
    conv_out_tvm = tvm.nd.array(out.shape,device=dev)

    #结果测试
    print("dataNum:", csr.data.shape)
    print("线程：",para)
    #评估时延
    timer = mod.time_evaluator(mod.entry_name, dev=dev, number=1000)
    result = (
        timer(x_tvm,
            weight_data_tvm,
            weight_indices_row_tvm,
            weight_indices_col_tvm,
            weight_indptr_tvm,
            resIdx_tvm,
            reshapeIdx_tvm,
            y,
            #   pad_out_tvm,
            #   trans_out_tvm
            #   conv_out_tvm
                ).mean
            * 1e3
        )
    print(
        "our Convolution: %f ms"
        % result
    )
    import psutil
    # 获取当前进程的信息
    current_process = psutil.Process()
    # 获取当前进程的线程数
    thread_count = current_process.num_threads()
    print("当前进程的线程数:", thread_count)  
    return result
    


# Save to the d2ltvm package.
def split_axis(factors, sch, op, axis):
        """Splitting an axis into factors

        Parameters
        ----------
        factors: array of integers
            The factors that the split applies
        sch: tvm.te.schedule.Schedule
            The tvm schedule
        op: tvm.te.tensor.Operation
            The stage to be applied
        axis: tvm.te.schedule.IterVar
            axis to split

        Returns
        -------
        axes : list of Axis
            The transformed axes.
        """
        ret = []
        for i in range(0, len(factors)):
            ax0, ax1 = sch[op].split(axis, factor=int(np.prod(factors[i:])))
            ret.append(ax0)
            axis = ax1
        return ret + [axis]




# def tiling(data_shape,weight_data_shape,weight_indices_shape,weight_indptr_shape,resIdx_shape,reshapeIdx_shape,kernel_size,strides,padding):
def tiling():
    # N, H, W, CO, CI, KH, KW, strides, padding = 1, 14, 14, 256, 256, 3, 3, (1, 1), (1, 1)
    N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 56, 56, 3, 3, (1, 1), (1, 1)
    kernel_size = KH
    sparity = 0.9
    para = 16
    # kernel_size*C,out_size*H

    tile_c = [1,4, 8]
    tile_h = [1,2, 2]
    tile_w = [2,16, 2]
    tile_rc = [1, 1]
    tile_rh = [1, 3]
    tile_rw = [1, 1]
    # for para in [8,16,32,64]:
    x = np.random.randn(N, CI, H, W).astype("float32")

    kernel = np.array(random_bsr_matrix(CO, CI*kernel_size*kernel_size, 1, 1, 0.1, "float32")
                    .todense()).reshape(CO, CI, kernel_size, kernel_size)

    #测试正确性
    # conv_np = conv2d_nchw_python(x, kernel, strides, padding)

    csr = utils.deal_sp_kernel(kernel)
    resIdx,reshapeIdx = dealKernel(csr,para)

    out_size = utils.conv_out_size(H,kernel_size,padding[0],strides[0])

    Xtrans = np.random.randn(kernel_size*CI,out_size*(H+2*padding[0])).astype("float32")

    # target = tvm.target.Target("llvm")
    target = tvm.target.Target("cuda")
    # dev = tvm.cpu()
    dev = tvm.cuda(0)
    # dev = tvm.gpu()
    out = tvm.nd.empty((N,CO,out_size,out_size), device=dev)
    # pad_out = tvm.nd.empty((N,CI,H+2*padding[0],W+2*padding[1]), device=dev)
    # trans_out = tvm.nd.empty((kernel_size*CI,out_size*(H+2*padding[0])), device=dev)

    #预处理
    x_row = CI * kernel_size
    weight_row = csr.indices % x_row
    weight_col = csr.indices // x_row * out_size

    print("#####################参数输出############################")
    print('weight_data:',csr.data.shape)
    print('weight_row:',weight_row.shape)
    print('weight_col:',weight_col.shape)
    print('weight_indptr:',csr.indptr.shape)
    print('稀疏度：', csr.data.shape[0]/(float)(CO*CI*kernel_size*kernel_size))
    print('输出尺寸：',out.shape)


    #数据形状获取
    data_shape = x.shape
    weight_data_shape = csr.data.shape
    weight_indices_shape = csr.indices.shape
    weight_indptr_shape = csr.indptr.shape
    resIdx_shape = resIdx.shape
    reshapeIdx_shape = reshapeIdx.shape

    input_mec, weight_data, weight_indices_row, weight_indices_col, weight_indptr,weight_resIdx, weight_reshapeIdx,Y = mec_csrmm(data_shape,
                                     weight_data_shape,
                                     weight_indices_shape,
                                     weight_indptr_shape,
                                     resIdx_shape,
                                     reshapeIdx_shape,
                                     kernel_size,
                                     strides,
                                     padding,
                                     dilation=1,)
    sch = te.create_schedule(Y.op)
    # sch[PaddedX].compute_inline()

    YL = sch.cache_write(Y, 'local')

    # create cache stage
    XX = sch.cache_read(input_mec, 'shared', [YL])
    # KK = sch.cache_read(weight_data, 'shared', [YL])
    XL = sch.cache_read(XX, 'local', [YL])
    # KL = sch.cache_read(KK, 'local', [YL])

    _,c, h, w = sch[Y].op.axis

    bc, vc, tc, ic = split_axis(tile_c, sch, Y, c)
    bh, vh, th, ih = split_axis(tile_h, sch, Y, h)
    bw, vw, tw, iw = split_axis(tile_w, sch, Y, w)

    sch[Y].bind(bc, te.thread_axis("blockIdx.z"))
    sch[Y].bind(bh, te.thread_axis("blockIdx.y"))
    sch[Y].bind(bw, te.thread_axis("blockIdx.x"))
    sch[Y].bind(vc, te.thread_axis("vthread"))
    sch[Y].bind(vh, te.thread_axis("vthread"))
    sch[Y].bind(vw, te.thread_axis("vthread"))
    sch[Y].bind(tc, te.thread_axis("threadIdx.z"))
    sch[Y].bind(th, te.thread_axis("threadIdx.y"))
    sch[Y].bind(tw, te.thread_axis("threadIdx.x"))
    sch[Y].reorder(bc, bh, bw, vc, vh, vw, tc, th, tw, ic, ih, iw)

    sch[YL].compute_at(sch[Y], tw)

    # tile reduction axes
    n,c, h, w = sch[YL].op.axis
    rc, = sch[YL].op.reduce_axis
    tk = 32
    ko, ki = sch[YL].split(rc, 32)
    # print(rc)
    # input()
    # rco, rcm, rci = split_axis(tile_rc, sch, YL, rc)
    # rho, rhm, rhi = split_axis(tile_rh, sch, YL, rh)
    # rwo, rwm, rwi = split_axis(tile_rw, sch, YL, rw)
    sch[YL].reorder(c, ko, ki, h, w)

    sch[XX].compute_at(sch[YL], ko)
    # sch[KK].compute_at(sch[YL], ko)
    sch[XL].compute_at(sch[YL], ki)
    # sch[KL].compute_at(sch[YL], ki)

    # cooperative fetching
    # for load in [XX, KK]:
    for load in [XX,]:
        args = sch[load].op.axis
        fused = sch[load].fuse(*args)
        # align thread layout
        tz, fused = sch[load].split(fused, nparts=tile_c[0])
        ty, fused = sch[load].split(fused, nparts=tile_h[0])
        tx, _ = sch[load].split(fused, nparts=tile_w[0])
        sch[load].bind(tz, te.thread_axis("threadIdx.z"))
        sch[load].bind(ty, te.thread_axis("threadIdx.y"))
        sch[load].bind(tx, te.thread_axis("threadIdx.x"))

    m = tvm.lower(sch, [input_mec,
                  weight_data,
                  weight_indices_row,
                  weight_indices_col,
                  weight_indptr,
                  weight_resIdx,
                  weight_reshapeIdx,
                #   output,
                  Y], name = 'test_mec')
    print(m)
    mod = tvm.build(m, target=target)
    x_tvm,weight_indptr_tvm,weight_indices_row_tvm,weight_indices_col_tvm,weight_data_tvm,y = (tvm.nd.array(i,device=dev) for i in(Xtrans,csr.indptr,weight_row,weight_col,csr.data,out))
    weight_row,weight_col = (tvm.nd.array(i,device=dev) for i in(weight_row,weight_col))
    resIdx_tvm = tvm.nd.array(resIdx,device=dev)
    reshapeIdx_tvm = tvm.nd.array(reshapeIdx,device=dev)
    conv_out_tvm = tvm.nd.array(out.shape,device=dev)
    print('#'*50)
    print("x_tvm:",x_tvm.shape)
    print("y:",y.shape)
    print("conv_out:",conv_out_tvm.shape)
    #结果测试
    print("dataNum:", csr.data.shape)
    print("线程：",para)
    #评估时延
    timer = mod.time_evaluator(mod.entry_name, dev=dev, number=1000)
    result = (
        timer(x_tvm,
            weight_data_tvm,
            weight_indices_row_tvm,
            weight_indices_col_tvm,
            weight_indptr_tvm,
            resIdx_tvm,
            reshapeIdx_tvm,
            y,
                ).mean
            * 1e3
        )
    print(
        "our Convolution: %f ms"
        % result
    )
import psutil
# 获取当前系统上的所有共享内存段
shms = [x for x in psutil.process_iter() if hasattr(x, 'numa')]
 
for shm in shms:
    print("共享内存 ID:", shm.pid)
    print("共享内存大小:", shm.memory_info().rss / (1024 * 1024), "MB")
    
input() 
m = tiling()

# #函数
# # input_mec, weight_data, weight_indices_row, weight_indices_col, weight_indptr,weight_resIdx, weight_reshapeIdx,output,padData,transData,con = mec_csrmm(data_shape,
# input_mec, weight_data, weight_indices_row, weight_indices_col, weight_indptr,weight_resIdx, weight_reshapeIdx,conv = mec_csrmm(data_shape,
#                                      weight_data_shape,
#                                      weight_indices_shape,
#                                      weight_indptr_shape,
#                                      resIdx_shape,
#                                      reshapeIdx_shape,
#                                      kernel_size,
#                                      strides,
#                                      padding,
#                                      dilation=1,)


# print("########################调度输出#########################")
# #调度conv
# print("########################CONV调度#########################")
# print("conv input tensor:",conv.op.input_tensors)
# print("conv.op.axis:", type(conv.op.axis), conv.op.axis)
# print("conv.op.reduce_axis:", type(conv.op.reduce_axis), conv.op.reduce_axis)

# output = conv
# # 获取 GPU 线程索引
# block_x = te.thread_axis("blockIdx.x")
# thread_x = te.thread_axis("threadIdx.x")

# #te创建调度
# s = te.create_schedule(output.op)


# # elem = s[conv].op.reduce_axis
# # s[conv].compute_inline()
# # s[conv].compute_root()
# # s[output].compute_inline()

# # s[con].compute_at(s[output],output.op.axis[0])


# # YL = s.cache_write(transData, 'shared')

# # XX = s.cache_read(input_mec,"shared",[YL])
# # XL = s.cache_read(XX, 'local', [YL])

# nt = 64
# outOP = conv
# x, y, z, zz = s[outOP].op.axis
# z = s[outOP].fuse(z, zz)
# y = s[outOP].fuse(y, z)
# fused = s[outOP].fuse(x, y)
# bx, tx = s[outOP].split(fused, factor=nt)

# # elem = s[output].op.reduce_axis
# # print("output reduce_axis:",output.op.reduce_axis)
# # s[output].reorder(bx,tx,elem)



# # s[output].bind(s[output].op.axis[2],block_x)
# s[outOP].bind(bx,block_x)
# # s[output].bind(s[output].op.axis[3],thread_x)
# s[outOP].bind(tx,thread_x)


# # outOP = output
# # x, y, z, zz = s[outOP].op.axis
# # z = s[outOP].fuse(z, zz)
# # y = s[outOP].fuse(y, z)
# # fused = s[outOP].fuse(x, y)
# # bx, tx = s[outOP].split(fused, factor=nt)
# # s[outOP].bind(bx,block_x)
# # s[outOP].bind(tx,thread_x)


# #TEsch结束后需要lower
# m = tvm.lower(s, [input_mec,
#                   weight_data,
#                   weight_indices_row,
#                   weight_indices_col,
#                   weight_indptr,
#                   weight_resIdx,
#                   weight_reshapeIdx,
#                 #   output,
#                   conv], name = 'test_mec')
# print(m)

# #写缓存
# # Cachedconv = s.cache_write(conv, 'local')
# # print("cacheconv.op.inputTensor:",Cachedconv.op.input_tensors)


# #调度conv
# print("########################CONV调度#########################")
# print("conv input tensor:",conv.op.input_tensors)
# print("conv.op.axis:", type(conv.op.axis), conv.op.axis)
# print("conv.op.reduce_axis:", type(conv.op.reduce_axis), conv.op.reduce_axis)

# #调度output
# # print("########################output调度#########################")
# # print("output input tensor:",output.op.input_tensors)
# # print("output.op.axis:", type(output.op.axis), output.op.axis)
# # p_c = s[output].fuse(output.op.axis[0],output.op.axis[1])
# # s[output].parallel(p_c)
# # u_c = s[output].fuse(output.op.axis[2],output.op.axis[3])
# # s[output].unroll(u_c)


# # s[output].bind(s[output].op.axis[2],block_x)
# # s[output].bind(s[output].op.axis[3],thread_x)

# # s[padd].bind(s[padd].op.axis[3],thread_x)
# # s[output].bind(u_c,block_x)


# mod = tvm.build(m, target=target)

# #TIRsch结束仍是IRModule
# # mod = tvm.build(m,target=target)
# # mod(x_tvm,weight_data_tvm,weight_indices_tvm,weight_indptr_tvm,y)
# # print(y)

# #数据转换

# # x_tvm,weight_indptr_tvm,weight_indices_row_tvm,weight_indices_col_tvm,weight_data_tvm,y = (tvm.nd.array(i,device=dev) for i in(x,csr.indptr,weight_row,weight_col,csr.data,out))
# x_tvm,weight_indptr_tvm,weight_indices_row_tvm,weight_indices_col_tvm,weight_data_tvm,y = (tvm.nd.array(i,device=dev) for i in(Xtrans,csr.indptr,weight_row,weight_col,csr.data,out))
# weight_row,weight_col = (tvm.nd.array(i,device=dev) for i in(weight_row,weight_col))

# resIdx_tvm = tvm.nd.array(resIdx,device=dev)
# reshapeIdx_tvm = tvm.nd.array(reshapeIdx,device=dev)
# # pad_out_tvm = tvm.nd.array(pad_out,device=dev)
# # trans_out_tvm = tvm.nd.array(trans_out,device=dev)
# conv_out_tvm = tvm.nd.array(out.shape,device=dev)

# print('#'*50)
# print("x_tvm:",x_tvm.shape)
# print("y:",y.shape)
# print("conv_out:",conv_out_tvm.shape)
# #结果测试
# print("dataNum:", csr.data.shape)
# print("线程：",para)
# #评估时延
# timer = mod.time_evaluator(mod.entry_name, dev=dev, number=1000)
# result = (
#     timer(x_tvm,
#           weight_data_tvm,
#           weight_indices_row_tvm,
#           weight_indices_col_tvm,
#           weight_indptr_tvm,
#           resIdx_tvm,
#           reshapeIdx_tvm,
#           y,
#         #   pad_out_tvm,
#         #   trans_out_tvm
#         #   conv_out_tvm
#             ).mean
#         * 1e3
#     )
# print(
#     "our Convolution: %f ms"
#     % result
# )
# # input("over")

# import psutil

# # 获取当前进程的信息
# current_process = psutil.Process()

# # 获取当前进程的线程数
# thread_count = current_process.num_threads()

# print("当前进程的线程数:", thread_count)
